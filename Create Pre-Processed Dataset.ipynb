{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the .wav format CHiME dataset, pre-processes it, and then saves it to file. This removes the pre-processing bottleneck from the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dataset\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "                'dataset': 'CHiME',\n",
    "                'sample_rate': 16384,  # Desired sample rate of audio. Input will be resampled to this\n",
    "                'n_fft': 1024,  # Number of samples in each fourier transform\n",
    "                'fft_hop': 256,  # Number of samples between the start of each fourier transform\n",
    "                'n_parallel_readers': 16,\n",
    "                'patch_window': 256,  # Number of fourier transforms (rows) in each patch\n",
    "                'patch_hop': 128,  # Number of fourier transforms between the start of each patch\n",
    "                'batch_size': 1,  # Number of patches in each batch\n",
    "                'n_shuffle': 1,  # Number of patches buffered before batching\n",
    "                'learning_rate': 0.0001,  # The learning rate to be used by the model\n",
    "                'epochs': 0,  # Number of full passes through the dataset to train for\n",
    "                'normalise_mag': True,  # Are magnitude spectrograms normalised in pre-processing?\n",
    "                'GPU': '0',\n",
    "                'local_run': False,\n",
    "                'chime_data_root': '/home/enterprise.internal.city.ac.uk/acvn728/NewCHiME/',\n",
    "                'librispeech_data_root': '/data/Speech_Data/LibriSpeech/'\n",
    "                }\n",
    "\n",
    "    \n",
    "train_data, val_data, test_data = dataset.prepare_datasets(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories to store the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('pre_processed_data'):\n",
    "    os.mkdir('pre_processed_data')\n",
    "for dataset in ['training','validation','test']:\n",
    "    path = os.path.join('pre_processed_data', dataset)\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start session\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.visible_device_list = str(model_config['GPU'])\n",
    "sess = tf.Session(config=tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterator = train_data.make_one_shot_iterator()\n",
    "batch = sess.run(train_iterator.get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "dump_location = 'pre_processed_data'\n",
    "dump_name = os.path.join(dump_location, str(counter))\n",
    "pickle.dump(batch, open(dump_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process and pickle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:406: UserWarning: An unusually high number of `Iterator.get_next()` calls was detected. This often indicates that `Iterator.get_next()` is being called inside a training loop, which will cause gradual slowdown and eventual resource exhaustion. If this is the case, restructure your code to call `next_element = iterator.get_next()` once outside the loop, and use `next_element` as the input to some computation that is invoked inside the loop.\n",
      "  warnings.warn(GET_NEXT_CALL_WARNING_MESSAGE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400 batches complete\n",
      "5500 batches complete\n",
      "5600 batches complete\n",
      "5700 batches complete\n",
      "5800 batches complete\n",
      "5900 batches complete\n",
      "6000 batches complete\n",
      "6100 batches complete\n",
      "6200 batches complete\n",
      "6300 batches complete\n",
      "6400 batches complete\n",
      "6500 batches complete\n",
      "6600 batches complete\n",
      "6700 batches complete\n",
      "6800 batches complete\n",
      "6900 batches complete\n",
      "7000 batches complete\n",
      "7100 batches complete\n",
      "7200 batches complete\n",
      "7300 batches complete\n",
      "7400 batches complete\n",
      "7500 batches complete\n",
      "7600 batches complete\n",
      "7700 batches complete\n",
      "7800 batches complete\n",
      "7900 batches complete\n",
      "8000 batches complete\n",
      "8100 batches complete\n",
      "8200 batches complete\n",
      "8300 batches complete\n",
      "8400 batches complete\n",
      "8500 batches complete\n",
      "8600 batches complete\n",
      "8700 batches complete\n",
      "8800 batches complete\n",
      "8900 batches complete\n",
      "9000 batches complete\n",
      "9100 batches complete\n",
      "9200 batches complete\n",
      "9300 batches complete\n",
      "9400 batches complete\n",
      "9500 batches complete\n",
      "9600 batches complete\n",
      "9700 batches complete\n",
      "9800 batches complete\n",
      "9900 batches complete\n",
      "10000 batches complete\n",
      "10100 batches complete\n",
      "10200 batches complete\n",
      "10300 batches complete\n",
      "10400 batches complete\n",
      "10500 batches complete\n",
      "10600 batches complete\n",
      "10700 batches complete\n",
      "10800 batches complete\n",
      "10900 batches complete\n",
      "11000 batches complete\n",
      "11100 batches complete\n",
      "11200 batches complete\n",
      "11300 batches complete\n",
      "11400 batches complete\n",
      "11500 batches complete\n",
      "11600 batches complete\n",
      "11700 batches complete\n",
      "11800 batches complete\n",
      "11900 batches complete\n",
      "12000 batches complete\n",
      "12100 batches complete\n",
      "12200 batches complete\n",
      "12300 batches complete\n",
      "12400 batches complete\n",
      "12500 batches complete\n",
      "12600 batches complete\n",
      "12700 batches complete\n",
      "12800 batches complete\n",
      "12900 batches complete\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "dump_location = 'pre_processed_data'\n",
    "#Test Data\n",
    "counter = 0\n",
    "test_iterator = test_data.make_one_shot_iterator()\n",
    "next_item = test_iterator.get_next()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        batch = sess.run(next_item)\n",
    "        dump_name = os.path.join(dump_location,'test', str(counter))\n",
    "        pickle.dump(batch, open(dump_name, 'wb'))\n",
    "        counter += 1\n",
    "        if counter % 100 == 0:\n",
    "            print('{c} batches complete'.format(c=counter))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('All done')\n",
    "        break\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_file(file):\n",
    "    \n",
    "    mixed_spec, voice_spec, background_spec, mixed_audio, voice_audio, background_audio = pickle.load(open(file, 'rb'))\n",
    "    \n",
    "    \n",
    "    return (mixed_spec, voice_spec, background_spec, mixed_audio, voice_audio, background_audio)\n",
    "\n",
    "def py_load_file(path):\n",
    "    \n",
    "    return tf.py_func(load_pickle_file, [path], tf.float32, stateful=False)\n",
    "\n",
    "\n",
    "def use_prepared_dataset(location):\n",
    "    \n",
    "    return (\n",
    "        tf.data.Dataset.list_files(location, shuffle=False)\n",
    "        .map(py_load_file)\n",
    "        #.shuffle(n_shuffle).batch(batch_size).prefetch(3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "pyfunc_80 returns 6 values, but expects to see 1 values.\n\t [[{{node PyFuncStateless}} = PyFuncStateless[Tin=[DT_STRING], Tout=[DT_FLOAT], token=\"pyfunc_80\"](arg0)]]\n\t [[{{node IteratorGetNext_5320}} = IteratorGetNext[output_shapes=[<unknown>], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_10)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: pyfunc_80 returns 6 values, but expects to see 1 values.\n\t [[{{node PyFuncStateless}} = PyFuncStateless[Tin=[DT_STRING], Tout=[DT_FLOAT], token=\"pyfunc_80\"](arg0)]]\n\t [[{{node IteratorGetNext_5320}} = IteratorGetNext[output_shapes=[<unknown>], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_10)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-386857440b71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnext_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: pyfunc_80 returns 6 values, but expects to see 1 values.\n\t [[{{node PyFuncStateless}} = PyFuncStateless[Tin=[DT_STRING], Tout=[DT_FLOAT], token=\"pyfunc_80\"](arg0)]]\n\t [[{{node IteratorGetNext_5320}} = IteratorGetNext[output_shapes=[<unknown>], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_10)]]"
     ]
    }
   ],
   "source": [
    "location = 'pre_processed_data/test/*'\n",
    "data = use_prepared_dataset(location)\n",
    "pipe = data.make_one_shot_iterator()\n",
    "next_item = pipe.get_next()\n",
    "sess.run(next_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
